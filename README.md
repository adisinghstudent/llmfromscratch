# llmfromscratch
attention is all you need, very basic from scratch training with guided steps!


Key Functions and Classes
EmbeddingLayer: Creates embeddings for token IDs.

Attention and MultiHeadAttention: Implements the attention mechanism.

TransformerLayer and Transformer: Builds and stacks Transformer blocks for deep sequence modeling.

GPT: Combines all components, managing training, loss calculation, and text generation.

generate: Handles autoregressive text generation based on a starting context.


