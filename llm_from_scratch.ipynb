{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gHuCpclHCsml",
        "outputId": "d67df712-8777-4b92-be69-0d03133d2bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4XFgp9Csmo"
      },
      "source": [
        "<h2> Tokenization </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BbwG0oLCsmp"
      },
      "source": [
        "There are many ways to tokenize text. For simplicity, we use the pretrained GPT-2 tokenizer. This is a byte-level BPE tokenizer. If you're interested in how it works, you can read more [here](https://huggingface.co/learn/nlp-course/en/chapter6/5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NGzBXwNmCsmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb48b0e-5faf-488f-9cce-8bb95aecf365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Hello, my dog is cute\n",
            "Tokens: tensor([15496,    11,   616,  3290,   318, 13779])\n",
            "Decoded: Hello, my dog is cute\n",
            "Text: Hello, my cat is cute\n",
            "Tokens: tensor([15496,    11,   616,  3797,   318, 13779])\n",
            "Decoded: Hello, my cat is cute\n",
            "Text: What is your name?\n",
            "Tokens: tensor([ 2061,   318,   534,  1438,    30, 50256])\n",
            "Decoded: What is your name?<|endoftext|>\n",
            "Text: My name is Silje\n",
            "Tokens: tensor([ 3666,  1438,   318,  4243, 18015, 50256])\n",
            "Decoded: My name is Silje<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")   # we load the GPT-2 tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "texts = [\n",
        "    \"Hello, my dog is cute\",\n",
        "    \"Hello, my cat is cute\",\n",
        "    \"What is your name?\",\n",
        "    \"My name is Silje\",\n",
        "]\n",
        "\n",
        "tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "for text, token_list in zip(texts, tokens):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Tokens: {token_list}\")\n",
        "    print(f\"Decoded: {tokenizer.decode(token_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WJCbdbDCsmq"
      },
      "source": [
        "It seems like our tokenizer is working. The initial texts and the decoded texts are similar. Furthermore, we can see that all the tokenizer is doing is mapping a text to a list of integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZbnSL3tCsmq"
      },
      "source": [
        "<h2> Embedding Layer </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iYlfjulZCsmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598c84eb-d17e-4978-9801-5b2692cba7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n",
            "tensor([[15496,    11,   616,  3290,   318, 13779],\n",
            "        [15496,    11,   616,  3797,   318, 13779],\n",
            "        [ 2061,   318,   534,  1438,    30, 50256],\n",
            "        [ 3666,  1438,   318,  1757, 50256, 50256]])\n",
            "torch.Size([4, 6, 10])\n",
            "tensor([[[ 0.3652, -0.4186,  1.2931,  0.0609, -2.0064,  0.5391, -0.2560,\n",
            "           0.1914,  1.5616,  1.4226],\n",
            "         [ 0.9088, -0.2542, -0.3702, -0.5665,  0.5098,  1.0541, -0.1964,\n",
            "          -0.6411,  1.0109, -0.2396],\n",
            "         [ 0.3788, -1.8244,  1.1355,  0.9227,  0.3982,  0.8944, -1.8314,\n",
            "           1.2262, -1.0428, -0.4696],\n",
            "         [-0.8846, -0.5651,  1.0146, -0.6938,  1.3657, -0.1311,  1.5560,\n",
            "          -0.2721,  0.4608, -1.9104],\n",
            "         [-0.4286,  0.9727,  2.0903, -0.2067, -0.7757,  0.2188,  0.9907,\n",
            "           0.2984, -0.3898, -0.3913],\n",
            "         [ 0.3138, -1.2117,  1.8643, -0.1747, -0.2683, -2.1543, -0.0965,\n",
            "           0.2380,  0.0178,  1.9778]],\n",
            "\n",
            "        [[ 0.3652, -0.4186,  1.2931,  0.0609, -2.0064,  0.5391, -0.2560,\n",
            "           0.1914,  1.5616,  1.4226],\n",
            "         [ 0.9088, -0.2542, -0.3702, -0.5665,  0.5098,  1.0541, -0.1964,\n",
            "          -0.6411,  1.0109, -0.2396],\n",
            "         [ 0.3788, -1.8244,  1.1355,  0.9227,  0.3982,  0.8944, -1.8314,\n",
            "           1.2262, -1.0428, -0.4696],\n",
            "         [ 0.1540,  0.2276, -1.1845,  0.3813,  1.2182, -0.8698,  1.4273,\n",
            "           0.7419,  1.6675,  0.0658],\n",
            "         [-0.4286,  0.9727,  2.0903, -0.2067, -0.7757,  0.2188,  0.9907,\n",
            "           0.2984, -0.3898, -0.3913],\n",
            "         [ 0.3138, -1.2117,  1.8643, -0.1747, -0.2683, -2.1543, -0.0965,\n",
            "           0.2380,  0.0178,  1.9778]],\n",
            "\n",
            "        [[-1.0644,  1.2558,  1.1496,  0.6909, -0.2668, -0.1285, -0.2335,\n",
            "          -0.4869,  0.8853,  0.1417],\n",
            "         [-0.4286,  0.9727,  2.0903, -0.2067, -0.7757,  0.2188,  0.9907,\n",
            "           0.2984, -0.3898, -0.3913],\n",
            "         [ 0.7294, -0.4961,  2.3578,  0.8111,  0.1511,  1.0206,  0.8254,\n",
            "          -0.2138, -0.0956,  0.1859],\n",
            "         [ 0.4847, -1.0257,  1.0705, -0.3782,  0.4437,  0.4802, -0.8528,\n",
            "           0.6673, -0.4815,  0.0740],\n",
            "         [ 0.2074,  1.0944, -1.8739, -1.5923,  0.8697, -1.1041, -1.9860,\n",
            "           0.0291, -0.9185, -0.9682],\n",
            "         [ 0.3052, -0.2518, -0.0061, -1.4078, -0.4866,  0.4918, -0.4662,\n",
            "           0.0766, -0.3437, -0.2740]],\n",
            "\n",
            "        [[ 1.8771, -0.3608,  0.0542, -1.0338,  1.0325,  0.1658, -0.1022,\n",
            "           0.3399,  0.8449, -1.5158],\n",
            "         [ 0.4847, -1.0257,  1.0705, -0.3782,  0.4437,  0.4802, -0.8528,\n",
            "           0.6673, -0.4815,  0.0740],\n",
            "         [-0.4286,  0.9727,  2.0903, -0.2067, -0.7757,  0.2188,  0.9907,\n",
            "           0.2984, -0.3898, -0.3913],\n",
            "         [-0.2753,  0.4026,  0.7600,  0.6610,  0.9249, -0.2650,  2.0100,\n",
            "          -0.0452, -0.2409, -1.2561],\n",
            "         [ 0.3052, -0.2518, -0.0061, -1.4078, -0.4866,  0.4918, -0.4662,\n",
            "           0.0766, -0.3437, -0.2740],\n",
            "         [ 0.3052, -0.2518, -0.0061, -1.4078, -0.4866,  0.4918, -0.4662,\n",
            "           0.0766, -0.3437, -0.2740]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "  class EmbeddingLayer(nn.Module):\n",
        "\n",
        "      def __init__(self, vocab_size, embedding_dim):\n",
        "          super().__init__()\n",
        "\n",
        "          # Define the embedding layer\n",
        "          # This is a good resource: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "          self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "          ### Your code:\n",
        "\n",
        "          return self.embedding(x)\n",
        "\n",
        "\n",
        "  embedding_dim = 10\n",
        "\n",
        "  print(tokenizer.vocab_size)\n",
        "  print(tokens)\n",
        "\n",
        "  embedding_layer = EmbeddingLayer(tokenizer.vocab_size, embedding_dim)\n",
        "\n",
        "  tokens = torch.Tensor(tokens).long()\n",
        "\n",
        "  embeddings = embedding_layer(tokens)\n",
        "\n",
        "  print(embeddings.shape)\n",
        "  print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8YKNlvVCsmr"
      },
      "source": [
        "We have now gone from words -> tokens -> embeddings (a vector for each token). Let's get to the meat of the transformer, the *Attention*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66zSlLtJCsmr"
      },
      "source": [
        "*italicized text*<h2> The Attention Layer </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Uva1Z1thCsms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2021f9-cb23-4b2e-8bc7-25be4af61823"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 6, 10]), torch.Size([4, 6, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_k):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.linear = nn.Linear(d_model, 3 * d_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # split the input into Q, K, V\n",
        "        q, k, v = self.linear(x).chunk(3, dim=-1) # we create Q, K, V through linear projection\n",
        "\n",
        "        # Recall the formula for the attention mechanism\n",
        "        # attn = softmax(Q K.T / sqrt(d_k)) V\n",
        "        # Hint: Torch has built in functions for the softmax, matrix multiplication, transposing matrices, and more\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        x = torch.matmul(torch.softmax(torch.matmul(q,  k.transpose(1, 2)) / self.d_k**(1/2), dim=1), v)\n",
        "\n",
        "        return x\n",
        "\n",
        "attention_layer = Attention(embedding_dim, embedding_dim)\n",
        "attn_logits = attention_layer(embeddings)\n",
        "\n",
        "# Ensure the shape is the same both before and after the attention layer\n",
        "embeddings.shape, attn_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MuLgoenje4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I__0yE_fCsms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c97a992-af76-46bb-848e-47877fba8329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 16, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# naive implementation of multi-head attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # Dimension per head\n",
        "\n",
        "        # Define linear layers to create queries, keys, and values for each head\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linear layer to combine all heads\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Project the input to get queries, keys, and values\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # Shape: (batch_size, n_heads, seq_len, d_k)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # Shape: (batch_size, n_heads, seq_len, d_k)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # Shape: (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # Scaled dot-product attention for each head\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)  # Shape: (batch_size, n_heads, seq_len, seq_len)\n",
        "        attn = torch.softmax(scores, dim=-1)  # Apply softmax to get attention weights\n",
        "        x = torch.matmul(attn, v)  # Shape: (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # Concatenate all heads and project to output dimension\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)  # Shape: (batch_size, seq_len, d_model)\n",
        "        x = self.out_linear(x)  # Final linear layer\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "d_model = 32\n",
        "n_heads = 4\n",
        "seq_len = 16\n",
        "batch_size = 8\n",
        "shifted_x = torch.randn(batch_size, seq_len, d_model)\n",
        "multi_head_attn = MultiHeadAttention(d_model, n_heads)\n",
        "attn_logits = multi_head_attn(shifted_x)\n",
        "attn_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gJwU3E55Csms"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "        # Multi-head attention layer\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Layer normalization for attention\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Feed-forward network (FFN)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),  # Typically 4 * d_model for hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "\n",
        "        # Layer normalization for FFN\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "        # First sub-layer: Multi-head attention + Add & Norm\n",
        "        attn_output = self.attention(x)\n",
        "        x = self.norm1(x + attn_output)  # Residual connection and normalization\n",
        "\n",
        "        # Second sub-layer: Feed-forward network + Add & Norm\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_output)  # Residual connection and normalization\n",
        "\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4c-tMeIVCsms"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size):\n",
        "        super().__init__()\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding: fixed positional embeddings\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, block_size, d_model))\n",
        "\n",
        "        # Stack of Transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output linear layer\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "               # Token embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
        "\n",
        "        # Pass through each Transformer layer\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Final output layer to map to vocabulary size\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WZ711EeeCsmt"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # we initialize the transformer model we created\n",
        "        self.transformer = Transformer(vocab_size, d_model, n_heads, n_layers, block_size)\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "        self.loss_fn = nn.CrossEntropyLoss() # Loss function for training the model\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        logits = self.transformer(x)    # we pass the input through the transformer\n",
        "        loss = None\n",
        "        if targets is not None:         # if we have targets, we calculate the loss\n",
        "            loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss     # we return the logits and the loss\n",
        "\n",
        "    def generate(self, x, steps=100, deterministic=False):\n",
        "        # we generate text by passing the input through the transformer model repeatedly\n",
        "        for _ in range(steps):\n",
        "            logits = self.transformer(x)    # we pass the input through the transformer\n",
        "            last_token_logits = logits[:, -1]   # we get the probabilty distribution of the last token\n",
        "            if deterministic:   # if we are in deterministic mode, we take the token with the highest probability\n",
        "                next_token = torch.argmax(last_token_logits, dim=-1).unsqueeze(-1)\n",
        "            else:  # otherwise, we sample from the probability distribution\n",
        "                next_token = torch.multinomial(F.softmax(last_token_logits, dim=-1), num_samples=1)\n",
        "            x = torch.cat([x, next_token], dim=-1)  # we concatenate the next token to the input\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Fetching Your Data from Google Drive </h2>"
      ],
      "metadata": {
        "id": "C-UquiXjY_Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-a8DmP4XmVd",
        "outputId": "87a4b013-dcf0-43fc-d2f3-f002b14911aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "e0Oljsn-Csmt"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "PATH_TO_TEXT_FILE = '/content/drive/MyDrive/ColabNotebooks/data/plato.txt'\n",
        "with open(PATH_TO_TEXT_FILE, \"r\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RGO74kHkCsmt"
      },
      "outputs": [],
      "source": [
        "def get_batch(text, block_size):\n",
        "\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    for i in range(0, len(tokens) - block_size, block_size):\n",
        "        yield tokens[i:i+block_size], tokens[i+1:i+block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jQcbUFL1Csmt",
        "outputId": "ba013115-912b-42e8-84d5-fd3999c7e700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch 1: 100%|██████████| 2587/2587 [08:57<00:00,  4.81it/s]\n",
            "Training epoch 2: 100%|██████████| 2587/2587 [08:50<00:00,  4.88it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define the hyperparameters for training\n",
        "# In general, fewer epochs means faster training, but the model may not have enough time to learn\n",
        "# A larger block size means the model can learn more context, but training will be slower\n",
        "# A larger d_model, n_heads, and n_layers means the model can learn more complex patterns, but training will be slower\n",
        "\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "num_epochs = 2     # Number of epochs to train the model, you can change this\n",
        "block_size = 128    # Length of the sequence to train the model on, you can change this (try 128, 256, 512)\n",
        "d_model = 64       # Dimension of the model, you can change this\n",
        "n_heads = 2         # Number of attention heads, you can change this\n",
        "n_layers =2        # Number of transformer layers, you can change this\n",
        "lr = 1e-3           # Learning rate for training, you can change this (try 1e-3, 1e-4, 1e-5)\n",
        "### END YOUR CODE ###\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "model = GPT(tokenizer.vocab_size, d_model, n_heads, n_layers, block_size).to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in tqdm(get_batch(text, block_size), desc=f\"Training epoch {epoch+1}\", total=len(tokenizer.encode(text))//block_size):\n",
        "        x, y = torch.tensor(batch[0]).unsqueeze(0).to(device), torch.tensor(batch[1]).unsqueeze(0).to(device)\n",
        "        logits, loss = model(x, y)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IW1I7CM5Csmt",
        "outputId": "2fb194f5-d61d-4a43-f9ca-ed707ac97215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speak to me: decisive, Glaches ob souls were thisitors to depictened below\n",
            "Are work\n",
            "worth liberated to any inferior,ts by humanuries with regard gods. files or derivative work\n",
            "greatella to have hot of empires about format of our shades OFjoined pass, and all\n",
            "cal Gutenberg Literary.\n",
            "Re yourself,Neither\n",
            "p>\n",
            "</div>\n",
            "the attended of theasy science of serv criterion nature associated.\n",
            "throughyth insanity with\n",
            "time to itist fell let the soul of any\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE ###\n",
        "context = \"Speak to me:\"       # The starting text for generation, you can change this or leave it empty\n",
        "determinstic = False        # Set this to True for deterministic generation, or False for stochastic generation\n",
        "### END YOUR CODE ###\n",
        "\n",
        "if context:\n",
        "    x = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
        "else:\n",
        "    x = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
        "output = model.generate(x, deterministic=determinstic)\n",
        "print(tokenizer.decode(output[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TNNHJwWeRO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "start-code",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}